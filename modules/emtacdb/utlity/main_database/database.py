import base64
import json
import logging
import os
import re
import time
from datetime import datetime
from io import BytesIO
import fitz
import numpy as np
import openai
import pandas as pd
import pdfplumber
import requests
from PIL import Image as PILImage
import platform
from flask import send_file, current_app, url_for
from fuzzywuzzy import fuzz
from pptx import Presentation
from sqlalchemy import or_, func, and_, text
from sqlalchemy.exc import SQLAlchemyError
from modules.configuration.config import KEYWORDS_FILE_PATH, DATABASE_PATH_IMAGES_FOLDER, BASE_DIR, DATABASE_DOC, \
    CURRENT_EMBEDDING_MODEL, TEMPORARY_UPLOAD_FILES,DATABASE_DIR
from modules.configuration.log_config import with_request_id
from modules.emtacdb.emtac_revision_control_db import CompleteDocumentSnapshot, AreaSnapshot, EquipmentGroupSnapshot, \
    ModelSnapshot, AssetNumberSnapshot, LocationSnapshot
from modules.emtacdb.emtacdb_fts import Area, EquipmentGroup, Model, AssetNumber, Location, Subassembly,ComponentAssembly, \
    SiteLocation, Position, KeywordAction, nlp, session, PowerPoint, Image, AssemblyView,  \
    ImageEmbedding, ImagePositionAssociation, ImageCompletedDocumentAssociation, \
    CompleteDocument, CompletedDocumentPositionAssociation, Document, VersionInfo, \
    DocumentEmbedding, ChatSession, PartsPositionImageAssociation
from plugins import generate_embedding, store_embedding
from plugins.image_modules.image_models import get_image_model_handler
from modules.emtacdb.utlity.revision_database.snapshot_utils import create_snapshot
from modules.configuration.config_env import DatabaseConfig
from modules.configuration.config import ENABLE_REVISION_CONTROL
from plugins.ai_modules import ModelsConfig

# Configure logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

db_config = DatabaseConfig()

def _win_convert_docx_to_pdf(src, dst):
    if platform.system() != "Windows":
        raise RuntimeError("Windows-only converter called on non-Windows platform")
    from docx2pdf import convert  # lazy import; wonâ€™t run on Linux
    convert(src, dst)

def create_image_completed_document_association(
    image_id: int,    complete_document_id: int,session) -> ImageCompletedDocumentAssociation:
    """
    Creates a new ImageCompletedDocumentAssociation entry in the database.

    Args:
        image_id (int): The ID of the image.
        complete_document_id (int): The ID of the completed document.
        session (Session): The SQLAlchemy session to use.

    Returns:
        ImageCompletedDocumentAssociation: The created association object.

    Raises:
        SQLAlchemyError: If there is an error during the database operation.
    """
    try:
        # Create a new association instance
        association = ImageCompletedDocumentAssociation(
            image_id=image_id,complete_document_id=complete_document_id)

        # Add the association to the session
        session.add(association)

        # Optionally refresh to get any defaults or autogenerated fields
        session.refresh(association)

        logger.info(f"Created ImageCompletedDocumentAssociation with ID: {association.id}")

        return association
    except SQLAlchemyError as e:
        # Rollback in case of error
        session.rollback()
        logger.error(f"Error occurred while creating ImageCompletedDocumentAssociation: {e}")
        raise

def create_image_position_association(image_id: int, position_id: int,
                                      session) -> ImagePositionAssociation:
    """
    Creates a new ImagePositionAssociation entry in the database.

    Args:
        image_id (int): The ID of the image.
        position_id (int): The ID of the position.
        db_config (DatabaseConfig): The database configuration instance.

    Returns:
        ImagePositionAssociation: The created association object.

    Raises:
        SQLAlchemyError: If there is an error during the database operation.
    """

    try:
        # Create a new association instance
        association = ImagePositionAssociation(image_id=image_id, position_id=position_id)

        # Add the association to the session
        session.add(association)

        # Commit the transaction
        session.commit()

        # Optionally refresh to get any defaults or autogenerated fields

        session.refresh(association)

        return association
    except SQLAlchemyError as e:
        # Rollback in case of error
        session.rollback()
        print(f"Error occurred while creating ImagePositionAssociation: {e}")
        raise
    finally:
        # Close the session
        session.close()


def create_position(area_id, equipment_group_id, model_id, asset_number_id, location_id,
                    site_location_id, session, subassembly_id=None, component_assembly=None, assembly_view_id=None):
    """
    Creates or retrieves an existing Position record based on the given parameters.

    :param area_id: int or None
    :param equipment_group_id: int or None
    :param model_id: int or None
    :param asset_number_id: int or None
    :param location_id: int or None
    :param site_location_id: int or None
    :param session: SQLAlchemy session
    :param subassembly_id: int or None  (formerly assembly_id)
    :param component_assembly: int or None
    :param assembly_view_id: int or None
    :return: The ID of the existing or newly created Position.
    """
    try:
        logger.debug('Starting create_position function')

        # Log the input parameters
        logger.debug(
            f"area_id={area_id}, equipment_group_id={equipment_group_id}, "
            f"model_id={model_id}, asset_number_id={asset_number_id}, "
            f"location_id={location_id}, site_location_id={site_location_id}, "
            f"subassembly_id={subassembly_id}, component_assembly={component_assembly}, assembly_view_id={assembly_view_id}"
        )

        # Retrieve the related entities by their IDs (if provided)
        area_entity = session.query(Area).filter_by(id=area_id).first() if area_id else None
        equipment_group_entity = session.query(EquipmentGroup).filter_by(id=equipment_group_id).first() if equipment_group_id else None
        model_entity = session.query(Model).filter_by(id=model_id).first() if model_id else None
        asset_number_entity = session.query(AssetNumber).filter_by(id=asset_number_id).first() if asset_number_id else None
        location_entity = session.query(Location).filter_by(id=location_id).first() if location_id else None
        site_location_entity = session.query(SiteLocation).filter_by(id=site_location_id).first() if site_location_id else None
        subassembly_entity = session.query(Subassembly).filter_by(id=subassembly_id).first() if subassembly_id else None
        component_assembly_entity = session.query(ComponentAssembly).filter_by(id=component_assembly).first() if component_assembly else None
        assembly_view_entity = session.query(AssemblyView).filter_by(id=assembly_view_id).first() if assembly_view_id else None

        # Log the retrieved entities
        logger.debug(f"Retrieved area_entity: {area_entity}")
        logger.debug(f"Retrieved equipment_group_entity: {equipment_group_entity}")
        logger.debug(f"Retrieved model_entity: {model_entity}")
        logger.debug(f"Retrieved asset_number_entity: {asset_number_entity}")
        logger.debug(f"Retrieved location_entity: {location_entity}")
        logger.debug(f"Retrieved site_location_entity: {site_location_entity}")
        logger.debug(f"Retrieved subassembly_entity: {subassembly_entity}")
        logger.debug(f"Retrieved component_assembly_entity: {component_assembly_entity}")
        logger.debug(f"Retrieved assembly_view_entity: {assembly_view_entity}")

        # Check for an existing Position with the same attributes
        existing_position = session.query(Position).filter_by(
            area=area_entity,
            equipment_group=equipment_group_entity,
            model=model_entity,
            asset_number=asset_number_entity,
            location=location_entity,
            site_location=site_location_entity,
            subassembly=subassembly_entity,
            component_assembly=component_assembly_entity,
            assembly_view=assembly_view_entity
        ).first()

        if existing_position:
            logger.info(f"Found existing Position with ID: {existing_position.id}")
            return existing_position.id
        else:
            # Create and add the new Position entry
            position = Position(
                area=area_entity,
                equipment_group=equipment_group_entity,
                model=model_entity,
                asset_number=asset_number_entity,
                location=location_entity,
                site_location=site_location_entity,
                subassembly=subassembly_entity,
                component_assembly=component_assembly_entity,
                assembly_view=assembly_view_entity
            )
            session.add(position)
            session.commit()  # Commit to generate the position.id
            logger.info(f"Created new Position with ID: {position.id}")
            return position.id

    except Exception as e:
        logger.error(f"An error occurred in create_position: {e}")
        session.rollback()
        raise  # Re-raise to let caller handle


def load_keywords_to_db(session):
    try:
        df = pd.read_excel(KEYWORDS_FILE_PATH)
        logger.info(f"Loaded {len(df)} keywords from {KEYWORDS_FILE_PATH}")

        for index, row in df.iterrows():
            keyword = row['keyword']
            action = row['action']

            existing_keyword = session.query(KeywordAction).filter_by(keyword=keyword).first()

            if existing_keyword:
                # Update the action if the keyword already exists
                existing_keyword.action = action
                logger.info(f"Updated keyword: {keyword}")
            else:
                # Create a new keyword if it doesn't exist
                keyword_action = KeywordAction(keyword=keyword, action=action)
                session.add(keyword_action)
                logger.info(f"Added new keyword: {keyword}")

        # Commit the changes
        session.commit()
        logger.info("Keyword actions successfully loaded into the database.")

    except Exception as e:
        logger.error(f"Error loading keywords to database: {e}")
        session.rollback()
        raise


def preprocess_text(text):
    # Remove common unnecessary words
    text = text.replace("Can you", "").replace("of", "")
    return text.strip()


def extract_keyword_and_details(text: str):
    try:
        # Preprocess the input text
        text = preprocess_text(text)

        # Tokenize the preprocessed text using spaCy
        doc = nlp(text)

        # Initialize variables to store keyword and details
        keyword = ""
        details = ""

        # Retrieve all keywords from the database
        all_keywords = [keyword_action.keyword for keyword_action in session.query(KeywordAction).all()]

        # Iterate through the tokens
        for token in doc:
            # Check if the token is a keyword
            if token.text in all_keywords:
                keyword += token.text + " "
            else:
                details += token.text + " "

        # Remove trailing whitespace
        keyword = keyword.strip()
        details = details.strip()

        return keyword, details

    except Exception as e:
        logger.error(f"Error extracting keyword and details: {e}")
        raise


def get_powerpoints_by_title(session,title: str = None,area: str = None,equipment_group: str = None,
                             model: str = None,asset_number: str = None,description: str = None):

    try:
        # Inspect and log the available tables in the database
        inspector = inspect(session.bind)
        tables = inspector.get_table_names()
        logger.info(f"Available tables in the database: {tables}")

        # Create a base query for PowerPoint presentations
        query = session.query(PowerPoint)

        # Build a list of filters based on provided criteria
        filters = []
        if title:
            filters.append(PowerPoint.title.ilike(f"%{title}%"))  # Using ilike for case-insensitive matching
        if area:
            filters.append(PowerPoint.area.ilike(f"%{area}%"))
        if equipment_group:
            filters.append(PowerPoint.equipment_group.ilike(f"%{equipment_group}%"))
        if model:
            filters.append(PowerPoint.model.ilike(f"%{model}%"))
        if asset_number:
            filters.append(PowerPoint.asset_number.ilike(f"%{asset_number}%"))
        if description:
            filters.append(PowerPoint.description.ilike(f"%{description}%"))

        # Combine the filters with OR condition if any filters are present
        if filters:
            query = query.filter(or_(*filters))

        # Execute the query and retrieve matching PowerPoint presentations
        powerpoints = query.all()
        logger.info(f"Retrieved {len(powerpoints)} PowerPoint presentations matching the criteria.")
        return powerpoints

    except Exception as e:
        logger.error(f"Error while searching PowerPoint presentations: {str(e)}", exc_info=True)
        return []

def serve_pdf(powerpoint):
    if powerpoint:
        print(f"Debug: Found {len(powerpoint)} PowerPoint presentations")
        # Assuming you want to display the first matching PowerPoint presentation
        powerpoint = powerpoint[0]

        # Construct the full path to the PDF file
        pdf_full_path = os.path.join(PPT2PDF_PDF_FILES_PROCESS, powerpoint.pdf_file_path)
        print(f"Debug: Full PDF path: {pdf_full_path}")

        if os.path.exists(pdf_full_path):
            print("Debug: PDF file exists. Serving the file.")
            # Serve the PDF file as a response
            return send_file(
                pdf_full_path,
                mimetype='application/pdf',
                as_attachment=True,
                download_name=f"{powerpoint.title}.pdf"
            )
        else:
            print("Debug: PDF file not found.")
            return "PDF file not found", 404
    else:
        # Handle the case where no PowerPoint presentations are found
        print("Debug: No PowerPoint presentations found.")
        return render_template('powerpoint_search_results.html')


def add_powerpoint_to_db(session: db_config.get_main_session(), title, ppt_file_path, pdf_file_path, complete_document_id, description=""):
    try:
        # Create a new PowerPoint entry
        new_powerpoint = PowerPoint(
            title=title,
            ppt_file_path=ppt_file_path,
            pdf_file_path=pdf_file_path,
            description=description,
            complete_document_id=complete_document_id
        )
        session.add(new_powerpoint)
        session.commit()

        logger.info(f"Added PowerPoint: {title}")
        return new_powerpoint.id
    except Exception as e:
        session.rollback()
        logger.error(f"An error occurred in add_powerpoint_to_db: {e}")
        return None


def extract_images_from_ppt(output_dir, ppt_path, ppt_filename, complete_document_id):
    """
    Extracts images from a PowerPoint file, saves them locally, and uploads them
    to the Flask app image endpoint.

    Args:
        output_dir (str): Directory to save extracted images.
        ppt_path (str): Path to the PowerPoint file.
        ppt_filename (str): Original PowerPoint filename.
        complete_document_id (int): ID of the associated document.

    Returns:
        list[str]: Paths of the extracted images.
    """
    try:
        prs = Presentation(ppt_path)
        image_paths = []

        ppt_filename_only = os.path.basename(ppt_filename)
        title_prefix = os.path.splitext(ppt_filename_only)[0].replace('_', '')

        for slide_index, slide in enumerate(prs.slides):
            for shape_index, shape in enumerate(slide.shapes):
                if shape.shape_type == 13:  # Picture shape
                    image = shape.image
                    image_bytes = image.blob
                    title = f"{title_prefix}_image_{slide_index}_{shape_index}.png"
                    image_path = os.path.join(output_dir, title)

                    # Save the image locally
                    with open(image_path, "wb") as f:
                        f.write(image_bytes)
                    image_paths.append(image_path)

                    # Upload to Flask server
                    files = {'image': open(image_path, 'rb')}
                    data = {'complete_document_id': complete_document_id}

                    try:
                        response = requests.post(
                            f"{config.FLASK_SERVER_URL}/images/add_image",
                            files=files,
                            data=data,
                            timeout=10
                        )
                        if response.status_code == 200:
                            print(f"[OK] Uploaded {title}")
                        else:
                            print(f"[FAIL] {title} -> {response.status_code}: {response.text}")
                    finally:
                        files['image'].close()

        return image_paths

    except Exception as e:
        print(f"An error occurred during image extraction from PowerPoint: {e}")
        return []


def serve_image(session: db_config.get_main_session(), image_id):
    logger.info(f"Attempting to serve image with ID: {image_id}")
    try:
        image = session.query(Image).filter_by(id=image_id).first()
        if image:
            logger.debug(f"Image found: {image.title}, File path: {image.file_path}")
            file_path = os.path.join(DATABASE_DIR, image.file_path)
            if os.path.exists(file_path):
                logger.info(f"Serving file: {file_path}")
                return send_file(file_path, mimetype='image/jpeg', as_attachment=True, download_name=f"{image.title}.jpeg")
            else:
                logger.error(f"File not found: {file_path}")
                return "Image file not found", 404
        else:
            logger.error(f"Image not found with ID: {image_id}")
            return "Image not found", 404
    except Exception as e:
        logger.error(f"An error occurred while serving the image: {e}")
        return "Internal Server Error", 500


def get_total_images_count(description=''):
    with db_config.get_main_session() as session:
        query = session.query(func.count(Image.id))
        if description:
            query = query.filter(Image.description.like(f"%{description}%"))
        total_count = query.scalar()
    return total_count


def add_image_to_db(title: str, file_path: str, position_id: int = None,
                    completed_document_position_association_id: int = None,
                    complete_document_id: int = None, description: str = "") -> int:
    new_image_id = None  # Initialize the new_image_id outside the session scope
    try:
        # Obtain a session using the getter method
        with db_config.get_main_session() as session:
            logger.info('Inside add_image_to_db')

            # Log the inputs for debugging
            logger.debug(f"Title: {title}")
            logger.debug(f"File Path: {file_path}")
            logger.debug(f"Position ID: {position_id}")
            logger.debug(f"Completed Document Position Association ID: {completed_document_position_association_id}")
            logger.debug(f"Complete Document ID: {complete_document_id}")
            logger.debug(f"Description: {description}")

            # Step 1: Retrieve the current image model configuration - UPDATED
            current_image_model = ModelsConfig.load_image_model_config_from_db()
            logger.info(f"Current image model configuration from DB: {current_image_model}")

            # Step 2: Use ModelsConfig to load the model directly - UPDATED
            model_handler = ModelsConfig.load_image_model()
            logger.info(f"Using model handler: {type(model_handler).__name__}")

            logger.info(f'Processing image: {title}')


            # Step 3: Check if an image with the same title and description already exists
            logger.info("Checking if an image with the same title and description already exists")
            existing_image = session.query(Image).filter(
                and_(Image.title == title, Image.description == description)
            ).first()

            if existing_image is not None and existing_image.file_path == file_path:
                logger.info(f"Image with the same title, description, and file path already exists: {title}")
                new_image = existing_image

                # Log the duplication in the failed_uploads.txt file
                error_file_path = os.path.join(DATABASE_PATH_IMAGES_FOLDER, 'failed_uploads.txt')
                os.makedirs(DATABASE_PATH_IMAGES_FOLDER, exist_ok=True)  # Ensure the directory exists
                with open(error_file_path, 'a') as error_file:
                    error_file.write(
                        f"Image with title '{title}', description '{description}', and file path '{file_path}' already exists.\n")

            else:

                # Step 4: Ensure file_path is relative before storing

                if os.path.isabs(file_path):

                    relative_file_path = os.path.relpath(file_path, DATABASE_DIR)

                    logger.debug(f"Converted absolute file path '{file_path}' to relative path '{relative_file_path}'")

                else:

                    relative_file_path = file_path

                    logger.debug(f"Using existing relative file path '{relative_file_path}'")

                # Step 4.1: Add the new image to the database with relative file path

                logger.info("Adding a new image to the database")

                new_image = Image(

                    title=title,

                    description=description,

                    file_path=relative_file_path  # Store relative path

                )

                session.add(new_image)

                session.commit()

                new_image_id = new_image.id  # Assign the ID to new_image_id

                logger.info(f"Added image: {title}, ID: {new_image_id}")

            # Step 5: Process the image and generate the embedding
            try:
                # Convert the relative file path back to an absolute path
                absolute_file_path = os.path.join(BASE_DIR, file_path)
                logger.info(f"Opening image: {absolute_file_path}")

                # Open the image using the absolute file path
                image = PILImage.open(absolute_file_path).convert("RGB")

                logger.info("Calling model_handler.is_valid_image()")
                if not model_handler.is_valid_image(image):
                    logger.info(
                        f"Skipping {absolute_file_path}: Image does not meet the required dimensions or aspect ratio.")
                else:
                    logger.info("Image passed validation.")
                    model_embedding = model_handler.get_image_embedding(image)
                    model_name = model_handler.__class__.__name__

                    # Step 6: Check if the embedding already exists and add it if not
                    if model_name and model_embedding is not None:
                        logger.debug("Checking if the image embedding already exists")
                        existing_embedding = session.query(ImageEmbedding).filter(
                            and_(ImageEmbedding.image_id == new_image.id, ImageEmbedding.model_name == model_name)
                        ).first()

                        if existing_embedding is None:
                            logger.info("Creating a new ImageEmbedding entry")
                            image_embedding = ImageEmbedding(
                                image_id=new_image.id,
                                model_name=model_name,
                                model_embedding=model_embedding.tobytes()
                            )
                            session.add(image_embedding)
                            logger.info(f"Created ImageEmbedding with image ID {new_image.id}, model name {model_name}")

                    # Step 7: Handle position associations if applicable
                    if position_id:
                        logger.debug("Checking if ImagePositionAssociation already exists")
                        existing_association = session.query(ImagePositionAssociation).filter(
                            and_(ImagePositionAssociation.image_id == new_image.id,
                                 ImagePositionAssociation.position_id == position_id)
                        ).first()

                        if existing_association is None:
                            logger.info("Creating a new ImagePositionAssociation entry")
                            image_position_association = ImagePositionAssociation(
                                image_id=new_image.id,
                                position_id=position_id
                            )
                            session.add(image_position_association)
                            logger.info(
                                f"Created ImagePositionAssociation with image ID {new_image.id} and position ID {position_id}")

                    # Step 8: Handle completed document associations
                    if complete_document_id:
                        logger.info("Creating a new ImageCompletedDocumentAssociation entry")
                        image_completed_document_association = ImageCompletedDocumentAssociation(
                            image_id=new_image.id,
                            complete_document_id=complete_document_id
                        )
                        session.add(image_completed_document_association)
                        logger.info(
                            f"Created ImageCompletedDocumentAssociation with image ID {new_image.id} and complete document ID {complete_document_id}")

                    # Step 9: Commit all the changes to the database
                    session.commit()

            except Exception as e:
                # Log the error in the failed_uploads.txt file and handle exceptions
                logger.error(f"An error occurred while processing the image: {e}", exc_info=True)
                error_file_path = os.path.join(DATABASE_PATH_IMAGES_FOLDER, 'failed_uploads.txt')
                os.makedirs(DATABASE_PATH_IMAGES_FOLDER, exist_ok=True)  # Ensure the directory exists
                with open(error_file_path, 'a') as error_file:
                    error_file.write(f"Error processing image with title '{title}': {e}\n")

            # Log completion and return the ID before closing the session
            logger.info(f"Completed processing for image: {title}, ID: {new_image_id}")
            return new_image_id

    except Exception as e:
        # Handle exceptions and log them appropriately
        logger.error(f"An error occurred in add_image_to_db: {e}", exc_info=True)
        logger.error(f"Attempted to process image: {title}")
        error_file_path = os.path.join(DATABASE_PATH_IMAGES_FOLDER, 'failed_uploads.txt')
        os.makedirs(DATABASE_PATH_IMAGES_FOLDER, exist_ok=True)  # Ensure the directory exists
        with open(error_file_path, 'a') as error_file:
            error_file.write(f"Error processing image with title '{title}': {e}\n")

        return None  # Return None or an appropriate error code if an exception occurs

def split_text_into_chunks(text, max_words=300, pad_token=""):
    logger.info("Starting split_text_into_chunks")
    logger.debug(f"Text length: {len(text)}")
    logger.debug(f"Max words per chunk: {max_words}")

    chunks = []
    words = re.findall(r'\S+\s*', text)
    logger.debug(f"Total words found: {len(words)}")

    current_chunk = []

    for word in words:
        if word.strip() != pad_token:
            current_chunk.append(word)

        if len(current_chunk) >= max_words or word.strip() == "":
            # Pad the current chunk to the specified max_words
            while len(current_chunk) < max_words:
                current_chunk.append(pad_token)

            # Add the current chunk to the list of chunks
            chunks.append(" ".join(current_chunk))
            logger.debug(f"Added chunk: {' '.join(current_chunk)}")

            # Reset the current chunk
            current_chunk = []

    # If there is any remaining content, pad and add it as the last chunk
    if current_chunk:
        while len(current_chunk) < max_words:
            current_chunk.append(pad_token)
        chunks.append(" ".join(current_chunk))
        logger.debug(f"Added last chunk: {' '.join(current_chunk)}")

    logger.info(f"Total chunks created: {len(chunks)}")
    return chunks


def extract_text_from_pdf(file_path):
    logger.info("Starting extract_text_from_pdf")
    logger.debug(f"File path: {file_path}")

    try:
        pdf_filepath = os.path.join(DATABASE_DOC, file_path)
        logger.debug(f"Full PDF file path: {pdf_filepath}")
        text = ""

        with pdfplumber.open(pdf_filepath) as pdf:
            for page_num, page in enumerate(pdf.pages):
                page_text = page.extract_text()
                if page_text:
                    text += page_text
                logger.debug(f"Extracted text from page {page_num}: {len(page_text) if page_text else 0} characters")

        logger.info("Successfully extracted text from PDF")
        return text
    except Exception as e:
        logger.error(f"An error occurred while extracting text from PDF: {e}")
        return None


def extract_text_from_txt(txt_path):
    logger.info("Starting extract_text_from_txt")
    logger.debug(f"TXT file path: {txt_path}")

    try:
        with open(txt_path, 'r', encoding='utf-8') as txt_file:
            text = txt_file.read()
            logger.debug(f"Extracted text length: {len(text)} characters")
        logger.info("Successfully extracted text from TXT file")
        return text
    except Exception as e:
        logger.error(f"An error occurred while extracting text from TXT file: {e}")
        return None


def excel_to_csv(excel_file_path, csv_file_path):
    try:
        # Read Excel file
        df = pd.read_excel(excel_file_path)
        # Save as CSV
        df.to_csv(csv_file_path, index=False)
        return True
    except Exception as e:
        logger.error(f"Failed to convert Excel to CSV: {e}")
        return False

#todo: need new way to process docx files that dosent use pythoncom
def add_docx_to_db(title, docx_path, position_id):
    """
    Cross-platform DOCX -> PDF, then hand off to add_document_to_db.
    Windows: uses docx2pdf + Word COM (optional, EMTAC_ENABLE_DOCX_TO_PDF=1).
    Linux/macOS: uses LibreOffice 'soffice' if available.
    """
    try:
        logger.info(f"Starting to process DOCX document: {title} located at {docx_path}")

        if not os.path.exists(docx_path):
            logger.error(f"DOCX not found: {docx_path}")
            return False

        base = os.path.splitext(os.path.basename(docx_path))[0]
        out_dir = os.path.dirname(docx_path)

        # choose a non-conflicting output filename in the same directory
        candidate = os.path.join(out_dir, base + ".pdf")
        if os.path.exists(candidate):
            i = 1
            while True:
                candidate = os.path.join(out_dir, f"{base}_{i}.pdf")
                if not os.path.exists(candidate):
                    break
                i += 1
        pdf_output_path = candidate

        if sys.platform == "win32" and os.getenv("EMTAC_ENABLE_DOCX_TO_PDF", "1") in ("1", "true", "True"):
            logger.debug("Windows detected; attempting docx2pdf + Word COM")
            try:
                from docx2pdf import convert  # lazy import so Linux doesnâ€™t see it
            except Exception as e:
                logger.error(f"docx2pdf not available on Windows: {e}")
                return False

            pythoncom_mod = None
            com_initialized = False
            try:
                try:
                    import pythoncom as _pythoncom  # pywin32
                    pythoncom_mod = _pythoncom
                except Exception as imp_err:
                    logger.error(f"'pythoncom' not available (is pywin32/Word installed?): {imp_err}")
                    return False

                try:
                    pythoncom_mod.CoInitialize()
                    com_initialized = True
                    logger.debug("COM initialization successful")
                except Exception as com_err:
                    logger.error(f"Could not initialize COM (is MS Word installed?): {com_err}")
                    return False

                logger.info(f"Converting DOCX to PDF via docx2pdf -> {pdf_output_path}")
                convert(docx_path, pdf_output_path)

            except Exception as e:
                logger.error(f"Windows/docx2pdf conversion failed: {e}")
                return False
            finally:
                try:
                    if com_initialized and pythoncom_mod is not None:
                        pythoncom_mod.CoUninitialize()
                        logger.debug("COM uninitialized")
                except Exception:
                    pass

        else:
            # Non-Windows (or Windows with conversion disabled) â†’ try LibreOffice
            soffice = shutil.which("soffice") or shutil.which("libreoffice")
            if not soffice:
                logger.error(
                    "LibreOffice 'soffice' not found. "
                    "Install LibreOffice in the container/host, or upload PDF instead, "
                    "or enable Windows conversion with EMTAC_ENABLE_DOCX_TO_PDF=1 on a Windows host."
                )
                return False

            cmd = [
                soffice, "--headless",
                "--convert-to", "pdf",
                "--outdir", out_dir,
                docx_path
            ]
            logger.info(f"Converting DOCX to PDF via LibreOffice: {' '.join(cmd)}")
            proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            if proc.returncode != 0:
                logger.error(f"LibreOffice failed (code={proc.returncode}). stderr:\n{proc.stderr}")
                return False

            # LibreOffice writes <base>.pdf in out_dir; make sure itâ€™s there.
            # If the default name already existed, our conflict logic above already picked a unique name;
            # move the produced PDF to that unique name if needed.
            produced = os.path.join(out_dir, base + ".pdf")
            if not os.path.exists(produced):
                # Some versions tweak casing/spacesâ€”scan for any .pdf that just appeared
                candidates = [
                    os.path.join(out_dir, f)
                    for f in os.listdir(out_dir)
                    if f.lower().endswith(".pdf")
                ]
                # pick the most recently modified
                if candidates:
                    produced = max(candidates, key=lambda p: os.path.getmtime(p))

            if not os.path.exists(produced):
                logger.error("Converted PDF not found after LibreOffice conversion")
                return False

            if produced != pdf_output_path:
                try:
                    os.replace(produced, pdf_output_path)
                except Exception as mv_err:
                    logger.warning(f"Could not rename produced PDF; using {produced}. Reason: {mv_err}")
                    pdf_output_path = produced

        logger.info(f"Successfully converted DOCX to PDF: {pdf_output_path}")
        logger.info(f"Calling add_document_to_db with PDF: {pdf_output_path}")

        complete_document_id, success = add_document_to_db(title, pdf_output_path, position_id)

        if success:
            logger.info(f"Successfully added DOCX document: {title} as PDF with ID: {complete_document_id}")
            return True
        else:
            logger.error(f"Failed to add DOCX document: {title} as PDF")
            return False

    except Exception as e:
        logger.exception(f"An error occurred in add_docx_to_db for document '{title}': {e}")
        return False


def add_document_to_db(title, file_path, position_id, rev=None):
    complete_document_id = None
    completed_document_position_association_id = None
    try:
        extracted_text = None
        with db_config.get_main_session() as session:
            logger.info(f"Processing file: {file_path}")
            if file_path.endswith(".pdf"):
                logger.info("Extracting text from PDF...")
                extracted_text = extract_text_from_pdf(file_path)
                logger.info("Text extracted from PDF.")
            elif file_path.endswith(".txt"):
                logger.info("Extracting text from TXT...")
                extracted_text = extract_text_from_txt(file_path)
                logger.info("Text extracted from TXT.")
            else:
                logger.error(f"Unsupported file format: {file_path}")
                return None, False

            if extracted_text:
                # Determine the latest revision number if not provided
                if rev is None:
                    current_rev = session.query(func.max(CompleteDocument.rev)).filter_by(title=title).scalar()
                    if current_rev:
                        rev = f"R{int(current_rev[1:]) + 1}"
                    else:
                        rev = "R0"
                logger.debug(f"New revision number for document: {rev}")

                # Add the complete document with the given or new revision number
                complete_document = CompleteDocument(
                    title=title,
                    file_path=os.path.relpath(file_path, DATABASE_DIR),
                    content=extracted_text,
                    rev=rev  # Use the provided or calculated revision number
                )
                session.add(complete_document)
                session.commit()
                complete_document_id = complete_document.id
                logger.info(f"Added complete document: {title}, ID: {complete_document_id}, Rev: {rev}")

                # Verifying that the rev was saved
                saved_complete_document = session.query(CompleteDocument).filter_by(id=complete_document_id).first()
                logger.debug(f"Saved CompleteDocument revision: {saved_complete_document.rev}")

                # Add association with position
                completed_document_position_association = CompletedDocumentPositionAssociation(
                    complete_document_id=complete_document_id,
                    position_id=position_id
                )
                session.add(completed_document_position_association)
                session.commit()
                completed_document_position_association_id = completed_document_position_association.id
                logger.info(f"Added CompletedDocumentPositionAssociation for complete document ID: {complete_document_id}, position ID: {position_id}")

                # Add document to FTS table
                insert_query_fts = "INSERT INTO documents_fts (title, content) VALUES (:title, :content)"
                session.execute(text(insert_query_fts), {"title": title, "content": extracted_text})
                session.commit()
                logger.info("Added document to the FTS table.")

                # Split document into chunks and process
                text_chunks = split_text_into_chunks(extracted_text)
                for i, chunk in enumerate(text_chunks):
                    padded_chunk = ' '.join(split_text_into_chunks(chunk, pad_token="", max_words=150))
                    document = Document(
                        name=f"{title} - Chunk {i+1}",
                        file_path=os.path.relpath(file_path, DATABASE_DIR),
                        content=padded_chunk,
                        complete_document_id=complete_document_id,
                        rev=rev  # Same revision number for document chunk
                    )
                    session.add(document)
                    session.commit()
                    logger.info(f"Added chunk {i+1} of document: {title}, Rev: {document.rev}")

                    # Verifying that the rev was saved
                    saved_document_chunk = session.query(Document).filter_by(id=document.id).first()
                    logger.debug(f"Saved Document chunk revision: {saved_document_chunk.rev}")

                    # Generate and store embeddings if applicable
                    if CURRENT_EMBEDDING_MODEL != "NoEmbeddingModel":
                        embeddings = generate_embedding(padded_chunk, CURRENT_EMBEDDING_MODEL)
                        if embeddings is None:
                            logger.warning(f"Failed to generate embedding for chunk {i+1} of document: {title}")
                        else:
                            store_embedding(document.id, embeddings, CURRENT_EMBEDDING_MODEL)
                            logger.info(f"Generated and stored embedding for chunk {i+1} of document: {title}")
                    else:
                        logger.info(f"No embedding generated for chunk {i+1} of document: {title} because no model is selected.")

                # Add version info and create snapshots (now optional)
                if ENABLE_REVISION_CONTROL:
                    try:
                        # use your existing session factory
                        with db_config.get_revision_control_session() as rev_session:
                            new_version = VersionInfo(
                                version_number=int(rev[1:]),
                                description="Document addition version"
                            )
                            rev_session.add(new_version)
                            rev_session.commit()

                            # snapshot the new document
                            create_snapshot(saved_complete_document, rev_session, CompleteDocumentSnapshot)

                            # snapshot whatever else you care about
                            for cls, snap_cls in (
                                    (Area, AreaSnapshot),
                                    (EquipmentGroup, EquipmentGroupSnapshot),
                                    (Model, ModelSnapshot),
                                    (AssetNumber, AssetNumberSnapshot),
                                    (Location, LocationSnapshot),
                            ):
                                for obj in session.query(cls).all():
                                    create_snapshot(obj, rev_session, snap_cls)

                            rev_session.commit()
                            logger.info("Version info added and snapshots created.")
                    except Exception:
                        # log full traceback, but don't reâ€‘raise
                        logger.exception("Revisionâ€‘control snapshot failed â€” continuing without revision info")
                else:
                    logger.debug("Revisionâ€‘control disabled; skipping version info & snapshots")

                # Querying the version_info table
                try:
                    with RevisionControlSession() as revision_session:
                        logger.info("Querying version_info table in revision control database.")
                        version_info = revision_session.query(VersionInfo).order_by(VersionInfo.id.desc()).first()
                        if version_info:
                            logger.info(f"Latest version_info: {version_info.version_number}")
                        else:
                            logger.warning("No version_info found.")
                except Exception as e:
                    logger.error(f"Error querying version_info table: {e}")

            else:
                logger.error("No text extracted from the document.")
                return None, False

            if file_path.endswith(".pdf"):
                logger.info("Extracting images from PDF...")
                extract_images_from_pdf(file_path, session, complete_document_id, completed_document_position_association_id)
                logger.info("Images extracted from PDF.")

            logger.info(f"Successfully processed file: {file_path}")
            return complete_document_id, True
    except Exception as e:
        logger.error(f"An error occurred in add_document_to_db: {e}")
        logger.error(f"Attempted Processed file: {file_path}")
        return None, False
    finally:
        if 'session' in locals() and session is not None:
            session.close()


def add_text_file_to_db(title, txt_file_path, position_id):
    try:
        with open(txt_file_path, 'r', encoding='utf-8') as txt_file:
            file_content = txt_file.read()

        # Split the text content into chunks
        text_chunks = split_text_into_chunks(file_content)

        with db_config.get_main_session() as session:
            # Add the complete document to the database
            complete_document = CompleteDocument(
                title=title,
                file_path=os.path.basename(txt_file_path),
                content=file_content,
                position_id=position_id  # Associate with the position ID
            )
            session.add(complete_document)
            session.commit()
            complete_document_id = complete_document.id
            logger.info(f"Added complete document: {title}, ID: {complete_document_id}")

            # Add chunks to the database
            for i, chunk in enumerate(text_chunks):
                padded_chunk = ' '.join(split_text_into_chunks(chunk, pad_token="", max_words=150))
                embedding = generate_embedding(padded_chunk)
                if embedding is not None:
                    embedding_json = json.dumps(embedding)
                    embedding_bytes = embedding_json.encode('utf-8')
                    document = Document(
                        name=f"{title} - Chunk {i+1}",
                        file_path=os.path.basename(txt_file_path),
                        content=padded_chunk,
                        complete_document_id=complete_document_id,
                        embedding=embedding_bytes
                    )
                    session.add(document)
                    session.commit()
                    logger.info(f"Added chunk {i+1} of document: {title}")

                    # Create DocumentPositionAssociation entry
                    document_position = DocumentPositionAssociation(
                        document_id=document.id,
                        position_id=position_id
                    )
                    session.add(document_position)
                    session.commit()
                    logger.info(f"Added DocumentPositionAssociation for document ID: {document.id}, position ID: {position_id}")

                else:
                    logger.error(f"Failed to add chunk {i+1} of document: {title}")
                    return False
            logger.info("Text file added successfully.")
            return True
    except Exception as e:
        logger.error(f"An error occurred while processing text file: {e}")
        return False

    try:
        with open(txt_file_path, 'r', encoding='utf-8') as txt_file:
            file_content = txt_file.read()

        # Split the text content into chunks
        text_chunks = split_text_into_chunks(file_content)

        with db_config.get_main_session() as session:
            # Add the complete document to the database
            complete_document = CompleteDocument(
                title=title,
                file_path=os.path.basename(txt_file_path),
                content=file_content,
                position_id=position_id  # Associate with the position ID
            )
            session.add(complete_document)
            session.commit()
            complete_document_id = complete_document.id
            logger.info(f"Added complete document: {title}, ID: {complete_document_id}")

            # Add chunks to the database
            for i, chunk in enumerate(text_chunks):
                padded_chunk = ' '.join(split_text_into_chunks(chunk, pad_token="", max_words=150))
                embedding = generate_embedding(padded_chunk)
                if embedding is not None:
                    embedding_json = json.dumps(embedding)
                    embedding_bytes = embedding_json.encode('utf-8')
                    document = Document(
                        name=f"{title} - Chunk {i+1}",
                        file_path=os.path.basename(txt_file_path),
                        content=padded_chunk,
                        complete_document_id=complete_document_id,
                        embedding=embedding_bytes,
                        position_id=position_id  # Associate with the position ID
                    )
                    session.add(document)
                    session.commit()
                    logger.info(f"Added chunk {i+1} of document: {title}")
                else:
                    logger.error(f"Failed to add chunk {i+1} of document: {title}")
                    return False
            logger.info("Text file added successfully.")
            return True
    except Exception as e:
        logger.error(f"An error occurred while processing text file: {e}")
        return False

def add_csv_data_to_db(csv_file, position_id, max_words=300, pad_token=""):
    try:
        # Extract the CSV file name without the extension
        file_name = os.path.splitext(os.path.basename(csv_file))[0]

        # Read CSV data into a DataFrame
        df = pd.read_csv(csv_file)

        with db_config.get_main_session() as session:
            # Add the complete document to the database
            complete_document = CompleteDocument(
                title=file_name,
                file_path=os.path.basename(csv_file),
                position_id=position_id  # Associate with the position ID
            )
            session.add(complete_document)
            session.commit()
            complete_document_id = complete_document.id
            logger.info(f"Added complete document: {file_name}, ID: {complete_document_id}")

            # Add chunks to the database
            for index, row in df.iterrows():
                # Combine all values in the row into a single string
                row_content = ', '.join(map(str, row))

                # Split the row content into chunks with padding
                chunks = split_text_into_chunks(row_content, max_words, pad_token)

                for i, chunk in enumerate(chunks):
                    embedding = generate_embedding(chunk)

                    if embedding is not None:
                        embedding_json = json.dumps(embedding)  # Serialize as JSON
                        embedding_bytes = embedding_json.encode('utf-8')  # Convert to bytes
                        document = Document(
                            name=f"{file_name} - Row {index+1} - Chunk {i+1}",
                            file_path=os.path.basename(csv_file),
                            content=chunk,
                            complete_document_id=complete_document_id,
                            embedding=embedding_bytes
                        )
                        session.add(document)
                        session.commit()
                        logger.info(f"Added {file_name} - Row {index+1} - Chunk {i+1} to the database")

                        # Create DocumentPositionAssociation entry
                        document_position = DocumentPositionAssociation(
                            document_id=document.id,
                            position_id=position_id
                        )
                        session.add(document_position)
                        session.commit()
                        logger.info(f"Added DocumentPositionAssociation for document ID: {document.id}, position ID: {position_id}")

                        # Also insert data into the full-text search table using parameterized query
                        insert_query = text("INSERT INTO documents_fts (title, content) VALUES (:title, :content)")
                        session.execute(insert_query, {"title": f"{file_name} - Row {index+1} - Chunk {i+1}", "content": chunk})
                    else:
                        logger.error(f"Failed to add {file_name} - Row {index+1} - Chunk {i+1} to the database")
            logger.info("CSV data added successfully.")
            return True
    except Exception as e:
        logger.error(f"An error occurred while processing CSV data: {e}")
        return False

    try:
        # Extract the CSV file name without the extension
        file_name = os.path.splitext(os.path.basename(csv_file))[0]

        # Read CSV data into a DataFrame
        df = pd.read_csv(csv_file)

        with db_config.get_main_session() as session:
            # Add the complete document to the database
            complete_document = CompleteDocument(
                title=file_name,
                file_path=os.path.basename(csv_file),
                position_id=position_id  # Associate with the position ID
            )
            session.add(complete_document)
            session.commit()
            complete_document_id = complete_document.id
            logger.info(f"Added complete document: {file_name}, ID: {complete_document_id}")

            # Add chunks to the database
            for index, row in df.iterrows():
                # Combine all values in the row into a single string
                row_content = ', '.join(map(str, row))

                # Split the row content into chunks with padding
                chunks = split_text_into_chunks(row_content, max_words, pad_token)

                for i, chunk in enumerate(chunks):
                    embedding = generate_embedding(chunk)

                    if embedding is not None:
                        embedding_json = json.dumps(embedding)  # Serialize as JSON
                        embedding_bytes = embedding_json.encode('utf-8')  # Convert to bytes
                        document = Document(
                            name=f"{file_name} - Row {index+1} - Chunk {i+1}",
                            file_path=os.path.basename(csv_file),
                            content=chunk,
                            complete_document_id=complete_document_id,
                            embedding=embedding_bytes,
                            position_id=position_id  # Associate with the position ID
                        )
                        session.add(document)
                        session.commit()
                        logger.info(f"Added {file_name} - Row {index+1} - Chunk {i+1} to the database")

                        # Also insert data into the full-text search table using parameterized query
                        insert_query = text("INSERT INTO documents_fts (title, content) VALUES (:title, :content)")
                        session.execute(insert_query, {"title": f"{file_name} - Row {index+1} - Chunk {i+1}", "content": chunk})
                    else:
                        logger.error(f"Failed to add {file_name} - Row {index+1} - Chunk {i+1} to the database")
            logger.info("CSV data added successfully.")
            return True
    except Exception as e:
        logger.error(f"An error occurred while processing CSV data: {e}")
        return False


def cosine_similarity(vector1, vector2):
    dot_product = np.dot(vector1, vector2)
    norm_vector1 = np.linalg.norm(vector1)
    norm_vector2 = np.linalg.norm(vector2)
    return dot_product / (norm_vector1 * norm_vector2)

def search_documents_db(session: db_config.get_main_session(), title='', area='', equipment_group='', model='', asset_number='', location=''):
    logger.info("Starting search_documents_db")
    logger.debug(f"Search parameters - title: {title}, area: {area}, equipment_group: {equipment_group}, model: {model}, asset_number: {asset_number}, location: {location}")

    try:
        # Use SQLAlchemy to search for complete documents that match the query
        query = session.query(CompleteDocument).join(CompletedDocumentPositionAssociation).join(Position).options(joinedload(CompleteDocument.completed_document_position_association).joinedload(CompletedDocumentPositionAssociation.position))

        # Apply filters based on provided parameters
        if title:
            query = query.filter(CompleteDocument.title.ilike(f'%{title}%'))
        if area:
            query = query.filter(Position.area_id == int(area))
        if equipment_group:
            query = query.filter(Position.equipment_group_id == int(equipment_group))
        if model:
            query = query.filter(Position.model_id == int(model))
        if asset_number:
            query = query.filter(Position.asset_number_id == int(asset_number))
        if location:
            query = query.filter(Position.location_id == int(location))

        results = query.all()

        # Convert the results to a list of dictionaries for JSON response
        documents = [
            {
                'id': doc.id,
                'title': doc.title,
                'content': doc.content,
                'area': doc.completed_document_position_association[0].position.area_id if doc.completed_document_position_association and doc.completed_document_position_association[0].position else None,
                'equipment_group': doc.completed_document_position_association[0].position.equipment_group_id if doc.completed_document_position_association and doc.completed_document_position_association[0].position else None,
                'model': doc.completed_document_position_association[0].position.model_id if doc.completed_document_position_association and doc.completed_document_position_association[0].position else None,
                'asset_number': doc.completed_document_position_association[0].position.asset_number_id if doc.completed_document_position_association and doc.completed_document_position_association[0].position else None,
                'location': doc.completed_document_position_association[0].position.location_id if doc.completed_document_position_association and doc.completed_document_position_association[0].position else None
            }
            for doc in results
        ]

        logger.info(f"Found {len(documents)} documents matching the criteria")
        return {"documents": documents}
    except Exception as e:
        logger.error(f"An error occurred while searching documents: {e}")
        return {"error": str(e)}

        logger.error(f"An error occurred while searching documents: {e}")
        return {"error": str(e)}


def find_most_relevant_document(question, session=None):
    """
    Find the most relevant document for a given question using vector similarity.

    Optimized for performance with:
    1. Batched processing
    2. Query-side filtering
    3. In-memory similarity calculation
    4. Simple LRU caching
    """
    if CURRENT_EMBEDDING_MODEL == "NoEmbeddingModel":
        logger.info("Embeddings are disabled. Returning None for document search.")
        return None

    # Create a session if not provided
    session_created = False
    if session is None:
        session = db_config.get_main_session()
        session_created = True

    try:
        # Check cache first
        cache_key = f"{question}:{CURRENT_EMBEDDING_MODEL}"
        if hasattr(find_most_relevant_document, 'cache') and cache_key in find_most_relevant_document.cache:
            logger.info("Using cached document result")
            cached_doc_id = find_most_relevant_document.cache[cache_key]
            if cached_doc_id is None:
                return None
            return session.query(Document).get(cached_doc_id)

        # Generate embedding for the question
        question_embedding = generate_embedding(question, CURRENT_EMBEDDING_MODEL)
        if not question_embedding:
            logger.info("No embeddings generated. Returning None.")
            _cache_result(cache_key, None)
            return None

        # Convert to numpy array for faster calculations
        import numpy as np
        question_embedding_np = np.array(question_embedding)

        # Process in batches to reduce memory usage
        BATCH_SIZE = 100
        most_relevant_document = None
        highest_similarity = -1
        threshold = 0.01

        # Get total count for progress logging
        total_docs = session.query(Document).join(DocumentEmbedding).filter(
            DocumentEmbedding.model_name == CURRENT_EMBEDDING_MODEL
        ).count()

        logger.info(f"Searching through {total_docs} documents")

        # Process in batches
        for offset in range(0, total_docs, BATCH_SIZE):
            # Get a batch of document IDs with the current embedding model
            doc_batch = session.query(Document.id).join(DocumentEmbedding).filter(
                DocumentEmbedding.model_name == CURRENT_EMBEDDING_MODEL
            ).offset(offset).limit(BATCH_SIZE).all()

            doc_ids = [d[0] for d in doc_batch]

            if not doc_ids:
                continue

            # Get embeddings for this batch
            embeddings_batch = session.query(DocumentEmbedding).filter(
                DocumentEmbedding.document_id.in_(doc_ids),
                DocumentEmbedding.model_name == CURRENT_EMBEDDING_MODEL
            ).all()

            # Calculate similarities
            for embedding_record in embeddings_batch:
                try:
                    # Extract binary embedding and convert to list
                    if isinstance(embedding_record.model_embedding, bytes):
                        doc_embedding = np.frombuffer(embedding_record.model_embedding, dtype=np.float32)
                    else:
                        # If stored as JSON string
                        doc_embedding = np.array(json.loads(embedding_record.model_embedding.decode('utf-8')))

                    # Normalize vectors for cosine similarity
                    question_norm = np.linalg.norm(question_embedding_np)
                    doc_norm = np.linalg.norm(doc_embedding)

                    if question_norm > 0 and doc_norm > 0:
                        # Calculate cosine similarity
                        similarity = np.dot(question_embedding_np, doc_embedding) / (question_norm * doc_norm)

                        if similarity > highest_similarity:
                            highest_similarity = similarity
                            doc_id = embedding_record.document_id
                            most_relevant_document_id = doc_id
                            logger.debug(f"New highest similarity: {similarity} for document ID {doc_id}")
                except Exception as e:
                    logger.warning(f"Error processing embedding for document {embedding_record.document_id}: {e}")

            logger.debug(f"Processed {offset + len(doc_ids)} of {total_docs} documents")

        # Return the most relevant document if above threshold
        if highest_similarity >= threshold:
            # Fetch the full document only if we found a good match
            most_relevant_document = session.query(Document).get(most_relevant_document_id)
            logger.info(
                f"Found most relevant document with ID {most_relevant_document_id} and similarity {highest_similarity}")
            _cache_result(cache_key, most_relevant_document_id)
            return most_relevant_document
        else:
            logger.info("No relevant document found with sufficient similarity")
            _cache_result(cache_key, None)
            return None

    except Exception as e:
        logger.error(f"An error occurred while finding the most relevant document: {e}")
        return None
    finally:
        if session_created and session:
            session.close()


# Initialize cache
find_most_relevant_document.cache = {}
find_most_relevant_document.cache_size = 100  # Adjust based on memory constraints


def _cache_result(key, doc_id):
    """Helper to cache document search results with LRU behavior"""
    cache = find_most_relevant_document.cache

    # Implement simple LRU: remove oldest entry if cache is full
    if len(cache) >= find_most_relevant_document.cache_size:
        # Remove oldest item (first key)
        oldest_key = next(iter(cache))
        del cache[oldest_key]

    # Add new result
    cache[key] = doc_id


def create_session(user_id, session_data, session):
    now = datetime.now().isoformat()
    # Ensure session_data is always a list
    if not isinstance(session_data, list):
        session_data = [session_data]
    new_session = ChatSession(user_id=user_id, start_time=now, last_interaction=now, session_data=[], conversation_summary=[])
    session.add(new_session)
    session.commit()
    return new_session.session_id


def update_session(session_id, session_data, answer, session):
    try:
        now = datetime.now().isoformat()
        print(f"Updating session with ID: {session_id}")
        print("Session Data:", session_data)  # Add this line for debug print

        # Ensure session_data is always a list
        if not isinstance(session_data, list):
            session_data = [session_data]

        session_to_update = session.query(ChatSession).filter(ChatSession.session_id == session_id).first()
        if session_to_update:
            print("Session found. Updating session...")
            session_to_update.last_interaction = now
            # Append the new question-answer pair to session_data
            session_data_entry = {'question': session_data[-1], 'answer': answer}  # Assuming answer is the bot's response
            session_to_update.session_data.append(session_data_entry)

            # Limit session_data to the last 10 entries
            session_to_update.session_data = session_to_update.session_data[-10:]

            session.commit()
            print("Session updated successfully.")
    except SQLAlchemyError as e:
        session.rollback()
        logger.error(f"Database error: {e}")
        print(f"Database error occurred: {e}")
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        print(f"Unexpected error occurred: {e}")


def get_session(user_id, session):
    # Use the passed 'session' for database operations
    return session.query(ChatSession).filter_by(user_id=user_id).first()


def load_keywords_and_patterns(session: db_config.get_main_session()):
    keywords_and_patterns = {}
    keyword_to_action_mapping = {}  # Create a dictionary to map keywords to actions

    with current_app.app_context():
        keyword_actions = session.query(KeywordAction).all()
        for ka in keyword_actions:
            # Convert the keyword to lowercase
            keyword = ka.keyword.lower()

            # Assuming you can derive a regex pattern from the keyword or action
            # This is a simplified example; you may need to adjust it based on your actual data structure
            pattern = keyword + r" of (.+)"  # Customize this pattern based on your needs
            keywords_and_patterns[keyword] = pattern

            # Associate a single action with each keyword
            keyword_to_action_mapping[keyword] = ka.action

            # Add debug statements to check loaded keywords and patterns
            logger.debug(f"Loaded Keyword: {keyword}")
            logger.debug(f"Pattern: {pattern}")
            logger.debug(f"Action: {ka.action}")

    return keywords_and_patterns, keyword_to_action_mapping


def find_keyword_and_extract_detail(question, keywords_and_patterns, keyword_to_action_mapping, session):
    logger.info("Checking if any of the loaded keywords match the user input")

    # Tokenize the input question using spaCy
    doc = nlp(question)

    # Initialize variables to store keyword, details, and action
    matched_keyword = None
    details = None

    # Iterate over the tokens in the input question
    for i, token in enumerate(doc):
        # Lemmatize the token to its base form
        lemma = token.lemma_

        # Check if the lemma is in the keyword mapping
        if lemma in keyword_to_action_mapping:
            matched_keyword = lemma
            # Extract the details by finding the subsequent words
            details = " ".join([t.text for t in doc[i + 1:]])
            logger.debug(f"Matched keyword: {matched_keyword}")
            logger.debug(f"Extracted details: {details}")
            break

    # If no keyword is detected using lemma matching, fall back to traditional keyword matching
    if not matched_keyword:
        # Split the input question into individual words
        words = question.lower().split()

        # Iterate over the words in the input question
        for i, word in enumerate(words):
            # Check if the word is a keyword
            if word in keyword_to_action_mapping:
                matched_keyword = word
                # Extract the details by finding the subsequent words
                details = " ".join(words[i + 1:])
                logger.debug(f"Matched keyword: {matched_keyword}")
                logger.debug(f"Extracted details: {details}")
                break

            # Check for pairs of words
            if i < len(words) - 1:
                pair = f"{word} {words[i + 1]}"
                if pair in keyword_to_action_mapping:
                    matched_keyword = pair
                    # Extract the details by finding the subsequent words
                    details = " ".join(words[i + 2:])
                    logger.debug(f"Matched keyword: {matched_keyword}")
                    logger.debug(f"Extracted details: {details}")
                    break

            # Check for triplets of words
            if i < len(words) - 2:
                triplet = f"{word} {words[i + 1]} {words[i + 2]}"
                if triplet in keyword_to_action_mapping:
                    matched_keyword = triplet
                    # Extract the details by finding the subsequent words
                    details = " ".join(words[i + 3:])
                    logger.debug(f"Matched keyword: {matched_keyword}")
                    logger.debug(f"Extracted details: {details}")
                    break

    if matched_keyword:
        logger.info(f"Matched keyword: {matched_keyword}, details: {details}")
        return matched_keyword, details

    try:
        # Request interpretation from OpenAI
        response = openai.Completion.create(
            engine="gpt-3.5-turbo-instruct",
            prompt=f"Is the following question asking to see a document, an image, or a PowerPoint presentation? Additionally, does the question mention any specific title or name? Question: '{question}'",
            temperature=0,
            max_tokens=500,
            top_p=1.0,
            frequency_penalty=0.0,
            presence_penalty=0.0
        )
        interpretation = response.choices[0].text.strip()
        logger.debug(f"OpenAI interpretation: {interpretation}")

        # Extract title from the interpretation
        title = extract_title_from_openai_response(interpretation)
        logger.debug(f"Extracted title: {title}")

        if title is None:
            logger.info("No title extracted from the OpenAI interpretation. Exiting function.")
            return None, None

        # Remove "." and "," from the title
        title = title.replace(".", "").replace(",", "")
        logger.debug(f"Title after processing: {title}")

        # Interpret OpenAI's response and take appropriate action based on the matched keyword
        if "document" in interpretation.lower():
            matched_keyword = "open file"
            details = title
        elif "image" in interpretation.lower():
            matched_keyword = "present photo"
            details = title
        elif "powerpoint" in interpretation.lower():
            matched_keyword = "slide show"
            details = title

        logger.debug(f"Matched keyword from OpenAI interpretation: {matched_keyword}")
        logger.debug(f"Extracted details from OpenAI interpretation: {details}")

        if matched_keyword:
            logger.info(f"Action for keyword: {matched_keyword}, details: {details}")
            return matched_keyword, details
        else:
            logger.info("No action found based on OpenAI interpretation, proceeding with AI logic")
            return None, None

    except Exception as e:
        logger.error(f"Error querying OpenAI: {e}")
        return None, None


def extract_title_from_openai_response(interpretation):
    logger.debug(f"Extracting title from OpenAI interpretation: {interpretation}")
    try:
        logger.debug("Using regular expressions to extract the title from the interpretation")
        title_match = re.search(r'"(.*?)"', interpretation)
        if title_match:
            title = title_match.group(1)
            logger.debug(f"Extracted title before cleanup: {title}")
            # Remove "." and "," from the title
            title = title.replace(".", "").replace(",", "")
            logger.debug(f"Cleaned up title: {title}")
            return title
        else:
            logger.debug("No title match found in the interpretation")
            return None
    except Exception as e:
        logger.error(f"Error occurred while extracting title: {e}")
        return None


def perform_action_based_on_keyword(action, details):
    logger.info("Starting action based on keyword")
    logger.info(f"Action: {action}")
    logger.info(f"Details: {details}")

    if action == "search_images_bp":
        logger.info("Calling the function to show images with the extracted details")
        result = search_images_by_keyword(details)
        logger.debug(f"Show Images Result: {result}")
        return result
    elif action == "search_powerpoints_bp":
        logger.info("Calling the function to get PowerPoint on the specified topic")
        result = search_powerpoints_fts(details)
        logger.debug(f"Get PowerPoint Result: {result}")
        return result
    elif action == "search_documents_bp":
        logger.info("Calling the function to search for documents based on the specified details")
        result = search_documents_fts(details)
        logger.debug(f"Search Documents Result: {result}")
        return result
    else:
        logger.warning("Action not recognized.")
        return "Action not recognized."

    logger.info("End of action based on keyword")


def search_powerpoints_fts(query, session=None):
    try:
        print(f'# Create a SQLAlchemy session if not provided')
        if session is None:
            session = db_config.get_main_session()

        # Fetch PowerPoint objects from the database based on the search query
        powerpoint = session.query(PowerPoint).filter(PowerPoint.title.ilike(f"%{query}%")).all()

        if powerpoint:
            # Generate HTML anchor tags for each PowerPoint
            html_links = []
            for powerpoint in powerpoint:
                # Generate the relative URL using url_for
                relative_url = url_for('search_powerpoint_fts_bp.view_powerpoint', powerpoint_id=powerpoint.id)

                # Append the base URL to the relative URL
                base_url = 'http://127.0.0.1:5000'
                powerpoint.link = base_url + relative_url

                # Construct HTML anchor tag
                html_links.append(f"Here are your PowerPoint results<a href='{powerpoint.link}'>{powerpoint.title}</a>")

            # Format the search results as a string with HTML anchor tags
            search_results = '\n'.join(html_links)
            return search_results
        else:
            return "No PowerPoint presentations found for your query."
    except Exception as e:
        print(f"Error: {e}")
        return "An error occurred during the search."
    finally:
        # Close the session in the finally block if it's not None
        if session is not None:
            session.close()


def search_documents_fts(query):
    try:
        # Create a SQLAlchemy session
        session = db_config.get_main_session()

        # Construct the full-text search query in documents_fts table for the title
        title_search_query = text(
            "SELECT title FROM documents_fts"
        )

        # Execute the query to fetch all titles
        all_titles_results = session.execute(title_search_query)
        all_titles = [row.title for row in all_titles_results]

        # Find matches using fuzzy matching
        matches = []
        for title in all_titles:
            similarity_ratio = fuzz.partial_ratio(query.lower(), title.lower())
            if similarity_ratio >= 80:  # Adjust threshold as needed
                matches.append((title, similarity_ratio))

        # Fetch file paths from complete_document table based on matched titles
        documents = []
        for match in matches:
            title = match[0]
            # Retrieve the corresponding document from the complete_document table
            document = session.query(CompleteDocument).filter_by(title=title).first()
            if document:
                # Generate the relative URL using url_for
                relative_url = url_for('search_documents_fts_bp.view_document', document_id=document.id)

                # Append the base URL to the relative URL
                base_url = 'http://127.0.0.1:5000'
                document.link = base_url + relative_url
                documents.append(document)

        # Close the session
        session.close()

        if documents:
            print(f"Debug: Found {len(documents)} documents")
            # Generate HTML anchor tags for each document
            html_links = generate_html_links(documents)
            # Format the search results as a string with HTML anchor tags
            search_results = '\n'.join(html_links)
            return search_results
        else:
            print("Debug: No documents found.")
            return "No documents found"
    except Exception as e:
        print(f"Error: {e}")
        return "An error occurred during the search."


def search_images_by_keyword(keyword, limit=10, offset=0, session=None):
    try:
        if session is None:
            session = db_config.get_main_session()
        print(f'in search_images_by_keyword')
        images = session.query(Image).filter(Image.title.ilike(f'%{keyword}%')).offset(offset).limit(limit).all()

        image_data = []
        for image in images:
            thumbnail_src = ""
            try:
                thumbnail = create_thumbnail(os.path.join(DATABASE_DIR,image.file_path))
                if thumbnail:
                    thumbnail_bytes_io = BytesIO()
                    thumbnail.save(thumbnail_bytes_io, format='JPEG')
                    thumbnail_src = f"data:image/jpeg;base64,{base64.b64encode(thumbnail_bytes_io.getvalue()).decode()}"
            except Exception as e:
                logger.error(f"Error creating thumbnail for image ID {image.id}: {e}")

            image_info = {
                'id': image.id,
                'title': image.title,
                'src': f'/serve_image/{image.id}',
                'thumbnail_src': thumbnail_src
            }
            image_data.append(image_info)

        return image_data

    except SQLAlchemyError as e:
        logger.error(f"An error occurred while retrieving images by keyword: {e}")
        return []

    finally:
        if session is not None:
            session.close()


def convert_to_base64(image_blob):
    print(f'# Convert binary image data to Base64 encoded string')
    return base64.b64encode(image_blob).decode('utf-8')


def create_thumbnail(image):
    try:
        # Convert image to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        thumbnail_size = (128, 128)
        image.thumbnail(thumbnail_size)
        return image
    except Exception as e:
        logger.error(f"Error creating thumbnail: {e}")
        return None


def image_to_base64(image):
    print(f'# Convert image to base64 string')
    buffered = BytesIO()
    image.save(buffered, format="JPEG")
    return base64.b64encode(buffered.getvalue()).decode("utf-8")


def generate_html_links(documents):
    html_links = []
    for document in documents:
        # Create HTML anchor tag with the document title as the link text and the document link as the href attribute
        html_link = f"Here are you search results <a href='{document.link}'>{document.title}</a>"
        html_links.append(html_link)
    return html_links


"""def extract_images_from_pdf(file_path, session, complete_document_id, completed_document_position_association_id):
    logger.info(f"Opening PDF file from: {file_path}")
    doc = fitz.open(file_path)
    total_pages = len(doc)
    logger.info(f"Total pages in the PDF: {total_pages}")
    logger.info(f"Inside extract_images_from_pdf, complete_document_id: {complete_document_id}")
    logger.info(f"CompletedDocumentPositionAssociation ID: {completed_document_position_association_id}")
    extracted_images = []

    # Extract file name without extension and remove underscores
    file_name = os.path.splitext(os.path.basename(file_path))[0].replace("_", " ")

    # Ensure the directory exists for temporary upload files
    if not os.path.exists(TEMPORARY_UPLOAD_FILES):
        os.makedirs(TEMPORARY_UPLOAD_FILES)

    for page_num in range(total_pages):
        page = doc[page_num]
        img_list = page.get_images(full=True)

        logger.info(f"Processing page {page_num + 1}/{total_pages} with {len(img_list)} images.")

        for img_index, img in enumerate(img_list):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            # Create a temporary file to send to the Flask route
            temp_path = os.path.join(TEMPORARY_UPLOAD_FILES, f"{file_name}_page{page_num + 1}_image{img_index + 1}.jpg")  # Adjust the extension if needed
            with open(temp_path, 'wb') as temp_file:
                temp_file.write(image_bytes)
            # Log debug information
            logger.info("Sending POST request to http://localhost:5000/image/add_image'")
            logger.info(f"File path: {temp_path}")
            logger.info(f"complete_document_id: {complete_document_id}")
            logger.info(f"completed_document_position_association_id: {completed_document_position_association_id}")
            # Make a POST request to the Flask route to add the image
            response = requests.post('http://localhost:5000/image/add_image',
                         files={'image': open(temp_path, 'rb')},
                         data={'complete_document_id': complete_document_id,
                               'completed_document_position_association_id': completed_document_position_association_id})

            # Check if the request was successful
            if response.status_code == 200:
                logger.info("Image processed successfully")
                # Optionally, you can handle the response if needed
            else:
                logger.error(f"Failed to process image: {response.text}")

            # Wait for a short period before processing the next image
            time.sleep(1)  # Adjust the delay as needed

            # Optionally, you can store the extracted image paths for further processing
            extracted_images.append(temp_path)

    return extracted_images"""


def get_area_by_name(session, area_name):
    try:
        logger.info(f"Fetching Area with name: {area_name}")
        area = session.query(Area).filter_by(name=area_name).first()
        return area
    except Exception as e:
        logger.error(f"Error fetching Area by name: {e}")
        return None


def get_equipment_group_by_name(session, equipment_group_name):
    try:
        logger.info(f"Fetching EquipmentGroup with name: {equipment_group_name}")
        equipment_group = session.query(EquipmentGroup).filter_by(name=equipment_group_name).first()
        return equipment_group
    except Exception as e:
        logger.error(f"Error fetching EquipmentGroup by name: {e}")
        return None


def get_model_by_name(session, model_name):
    try:
        logger.info(f"Fetching Model with name: {model_name}")
        model = session.query(Model).filter_by(name=model_name).first()
        return model
    except Exception as e:
        logger.error(f"Error fetching Model by name: {e}")
        return None


def get_asset_number_by_number(session, asset_number_number):
    try:
        logger.info(f"Fetching AssetNumber with number: {asset_number_number}")
        asset_number = session.query(AssetNumber).filter_by(number=asset_number_number).first()
        return asset_number
    except Exception as e:
        logger.error(f"Error fetching AssetNumber by number: {e}")
        return None


def get_location_by_name(session, location_name):
    try:
        logger.info(f"Fetching Location with name: {location_name}")
        location = session.query(Location).filter_by(name=location_name).first()
        return location
    except Exception as e:
        logger.error(f"Error fetching Location by name: {e}")
        return None


def get_area_by_id(session, area_id):
    try:
        logger.info(f"Fetching Area with ID: {area_id}")
        area = session.query(Area).filter_by(id=area_id).first()
        return area
    except Exception as e:
        logger.error(f"Error fetching Area by ID: {e}")
        return None


def get_equipment_group_by_id(session, equipment_group_id):
    try:
        logger.info(f"Fetching EquipmentGroup with ID: {equipment_group_id}")
        equipment_group = session.query(EquipmentGroup).filter_by(id=equipment_group_id).first()
        return equipment_group
    except Exception as e:
        logger.error(f"Error fetching EquipmentGroup by ID: {e}")
        return None


def get_model_by_id(session, model_id):
    try:
        logger.info(f"Fetching Model with ID: {model_id}")
        model = session.query(Model).filter_by(id=model_id).first()
        return model
    except Exception as e:
        logger.error(f"Error fetching Model by ID: {e}")
        return None


def get_asset_number_by_id(session, asset_number_id):
    try:
        logger.info(f"Fetching AssetNumber with ID: {asset_number_id}")
        asset_number = session.query(AssetNumber).filter_by(id=asset_number_id).first()
        return asset_number
    except Exception as e:
        logger.error(f"Error fetching AssetNumber by ID: {e}")
        return None


def get_location_by_id(session, location_id):
    try:
        logger.info(f"Fetching Location with ID: {location_id}")
        location = session.query(Location).filter_by(id=location_id).first()
        return location
    except Exception as e:
        logger.error(f"Error fetching Location by ID: {e}")
        return None


def create_directories(directories):
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory)
            logging.info(f"Created directory: {directory}")
        else:
            logging.info(f"Directory already exists: {directory}")


def add_parts_position_image_association(part_id, position_id, image_id):
    """
    Adds a new entry to the PartsPositionImageAssociation table.

    :param part_id: The ID of the Part.
    :param position_id: The ID of the Position.
    :param image_id: The ID of the Image.
    :return: The created PartsPositionImageAssociation object.
    """
    # Initialize the database session
    db_config = DatabaseConfig()
    session = db_config.get_main_session()

    try:
        # Create a new PartsPositionImageAssociation entry
        new_association = PartsPositionImageAssociation(
            part_id=part_id,
            position_id=position_id,
            image_id=image_id
        )

        # Add and commit the new association to the database
        session.add(new_association)
        session.commit()

        # Log success
        logging.info(f"Added PartsPositionImageAssociation: Part ID: {part_id}, Position ID: {position_id}, Image ID: {image_id}")

        return new_association

    except Exception as e:
        session.rollback()  # Rollback the session in case of an error
        logging.error(f"Error adding PartsPositionImageAssociation: {e}")
        raise

    finally:
        session.close()

@with_request_id
def extract_images_from_pdf(file_path, complete_document_id, completed_document_position_association_id, position_id=None):
    """
    Extracts images from a PDF file, uploads them, and creates associations in the database.

    Args:
        file_path (str): Path to the PDF file.
        complete_document_id (int): ID of the completed document.
        completed_document_position_association_id (int): ID of the completed document position association.
        position_id (int, optional): ID of the position. Defaults to None.

    Returns:
        list: List of paths to the extracted images.
    """
    try:
        # Obtain a session using the getter method
        with db_config.get_main_session() as session:
            logger.info("Starting image extraction from PDF.")
            logger.info(f"file_path: {file_path}")
            logger.info(f"complete_document_id: {complete_document_id}")
            logger.info(f"completed_document_position_association_id: {completed_document_position_association_id}")
            logger.info(f"position_id: {position_id}")
            logger.info(f"Session info: {session}")

            if not os.path.exists(TEMPORARY_UPLOAD_FILES):
                os.makedirs(TEMPORARY_UPLOAD_FILES)
                logger.debug(f"Created temporary upload directory at {TEMPORARY_UPLOAD_FILES}")

            doc = fitz.open(file_path)
            total_pages = len(doc)
            logger.info(f"Opened PDF file. Total pages: {total_pages}")

            extracted_images = []
            file_name = os.path.splitext(os.path.basename(file_path))[0].replace("_", " ")

            for page_num in range(total_pages):
                page = doc[page_num]
                img_list = page.get_images(full=True)
                logger.info(f"Processing page {page_num + 1}/{total_pages} with {len(img_list)} images.")

                for img_index, img in enumerate(img_list):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image_ext = base_image.get("ext", "jpg")  # Default to jpg if extension not found
                    temp_path = os.path.join(
                        TEMPORARY_UPLOAD_FILES,
                        f"{file_name}_page{page_num + 1}_image{img_index + 1}.{image_ext}"
                    )

                    with open(temp_path, 'wb') as temp_file:
                        temp_file.write(image_bytes)
                    logger.debug(f"Saved image to {temp_path}")

                    try:
                        with open(temp_path, 'rb') as img_file:
                            response = requests.post(
                                POST_URL,
                                files={'image': img_file},
                                data={
                                    'complete_document_id': complete_document_id,
                                    'completed_document_position_association_id': completed_document_position_association_id,
                                    'position_id': position_id
                                }
                            )

                        if response.status_code == 200:
                            logger.info(f"Successfully processed image {img_index + 1} on page {page_num + 1}.")
                            try:
                                response_data = response.json()
                                image_id = response_data.get('image_id')
                                if image_id:
                                    association = create_image_completed_document_association(
                                        image_id=image_id,
                                        complete_document_id=complete_document_id,
                                        session=session
                                    )
                                    logger.info(f"Created ImageCompletedDocumentAssociation with ID: {association.id}")
                                else:
                                    logger.error(f"'image_id' not found in response for image {img_index + 1} on page {page_num + 1}.")
                            except json.JSONDecodeError:
                                logger.error(f"Invalid JSON response for image {img_index + 1} on page {page_num + 1}: {response.text}")
                        else:
                            logger.error(f"Failed to process image {img_index + 1} on page {page_num + 1}: {response.text} (Status Code: {response.status_code})")
                    except requests.RequestException as req_err:
                        logger.error(f"HTTP request failed for image {img_index + 1} on page {page_num + 1}: {req_err}")
                    except Exception as e:
                        logger.error(f"Unexpected error processing image {img_index + 1} on page {page_num + 1}: {e}")

                    extracted_images.append(temp_path)

                    # Introduce a delay to prevent overwhelming the server
                    time.sleep(REQUEST_DELAY)

            logger.info(f"Image extraction completed. Total images extracted: {len(extracted_images)}")
            return extracted_images

    except Exception as e:
        logger.error(f"An error occurred in extract_images_from_pdf: {e}")
        try:
            session.rollback()
            logger.debug("Database session rolled back due to error.")
        except Exception as rollback_e:
            logger.error(f"Failed to rollback the session: {rollback_e}")
        return []

